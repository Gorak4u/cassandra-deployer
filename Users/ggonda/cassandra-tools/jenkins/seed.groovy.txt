// This seed job reads all Jenkinsfile definitions from this script and creates
// a separate, parameterized pipeline job for each one.
// It assumes the project is checked out or available at /Users/ggonda/cassandra-tools on the Jenkins machine.

def folderName = 'Cassandra'
def projectPath = '/Users/ggonda/cassandra-tools'

// Ensure the folder exists
folder(folderName)

// =============================================================================
// Helper function to create a pipeline job from a script text
// =============================================================================
def createPipelineJob(String jobName, String pipelineScript) {
    pipelineJob("${folderName}/${jobName}") {
        // This makes the jobs show up nicely in the Blue Ocean UI
        metaClass.isBlueOcean = { -> return true }

        // The entire pipeline definition is provided as a string
        definition {
            cps {
                script(pipelineScript)
                sandbox()
            }
        }
    }
}


// =============================================================================
// Job Definitions
// =============================================================================

// --- Generic Command Job ---
def commandJobScript = """
pipeline {
    agent any

    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'QV_QUERY', defaultValue: '-r role_cassandra_pfpt', description: 'The QV query to select nodes.')
        string(name: 'NODE_LIST', defaultValue: '', description: 'A comma-separated list of nodes.')
        string(name: 'CASSY_COMMAND', defaultValue: 'sudo cass-ops health', description: 'The command to run via cassy.sh.')
        booleanParam(name: 'PARALLEL', defaultValue: false, description: 'Run in parallel on all nodes?')
    }

    stages {
        stage('Execute Command') {
            steps {
                dir('${projectPath}') {
                    script {
                        def node_arg = ""
                        if (params.NODE_SELECTION_METHOD == 'QV_QUERY') {
                            node_arg = "--qv-query \\"${params.QV_QUERY}\\""
                        } else {
                            node_arg = "--nodes \\"${params.NODE_LIST}\\""
                        }
                        
                        def parallel_flag = params.PARALLEL ? '--parallel' : ''

                        sh """
                            chmod +x ./scripts/cassy.sh
                            ./scripts/cassy.sh \${node_arg} -c "\${params.CASSY_COMMAND}" \${parallel_flag}
                        """
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Command', commandJobScript)


// --- Rolling Restart Job ---
def restartJobScript = """
pipeline {
    agent any
    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'QV_QUERY', defaultValue: '-r role_cassandra_pfpt -d AWSLAB', description: 'The QV query to select nodes for restart.')
        string(name: 'NODE_LIST', defaultValue: '', description: 'A comma-separated list of nodes for restart.')
    }
    stages {
        stage('Execute Rolling Restart') {
            steps {
                dir('${projectPath}') {
                    script {
                        def node_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--qv-query \\"${params.QV_QUERY}\\"" : "--nodes \\"${params.NODE_LIST}\\""
                        sh "chmod +x ./scripts/cassy.sh && ./scripts/cassy.sh --rolling-op restart \${node_arg}"
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Restart', restartJobScript)

// --- Rolling Reboot Job ---
def rebootJobScript = """
pipeline {
    agent any
    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'QV_QUERY', defaultValue: '-r role_cassandra_pfpt -d AWSLAB', description: 'The QV query to select nodes for reboot.')
        string(name: 'NODE_LIST', defaultValue: '', description: 'A comma-separated list of nodes for reboot.')
    }
    stages {
        stage('Execute Rolling Reboot') {
            steps {
                dir('${projectPath}') {
                    script {
                        def node_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--qv-query \\"${params.QV_QUERY}\\"" : "--nodes \\"${params.NODE_LIST}\\""
                        sh "chmod +x ./scripts/cassy.sh && ./scripts/cassy.sh --rolling-op reboot \${node_arg}"
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Reboot', rebootJobScript)

// --- Rolling Puppet Run Job ---
def puppetRunJobScript = """
pipeline {
    agent any
    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'QV_QUERY', defaultValue: '-r role_cassandra_pfpt -d AWSLAB', description: 'The QV query to select nodes for puppet run.')
        string(name: 'NODE_LIST', defaultValue: '', description: 'A comma-separated list of nodes for puppet run.')
    }
    stages {
        stage('Execute Rolling Puppet Run') {
            steps {
                dir('${projectPath}') {
                    script {
                        def node_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--qv-query \\"${params.QV_QUERY}\\"" : "--nodes \\"${params.NODE_LIST}\\""
                        sh "chmod +x ./scripts/cassy.sh && ./scripts/cassy.sh --rolling-op puppet \${node_arg}"
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Puppet-run', puppetRunJobScript)

// --- Join Datacenters Job ---
def joinDcsJobScript = """
pipeline {
    agent any
    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'OLD_DC_QUERY', defaultValue: '-r role_cassandra_pfpt -d us-east-1', description: 'QV Query for OLD datacenter nodes.')
        string(name: 'NEW_DC_QUERY', defaultValue: '-r role_cassandra_pfpt -d eu-west-1', description: 'QV Query for NEW datacenter nodes.')
        string(name: 'OLD_DC_NODES', defaultValue: '', description: 'Node list for OLD datacenter.')
        string(name: 'NEW_DC_NODES', defaultValue: '', description: 'Node list for NEW datacenter.')
        string(name: 'OLD_DC_NAME', defaultValue: 'us-east-1', description: 'Cassandra name of the OLD datacenter.')
        string(name: 'NEW_DC_NAME', defaultValue: 'eu-west-1', description: 'Cassandra name of the NEW datacenter.')
    }
    stages {
        stage('Execute Join Operation') {
            steps {
                dir('${projectPath}') {
                    script {
                        def old_dc_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--old-dc-query \\"${params.OLD_DC_QUERY}\\"" : "--old-dc-nodes \\"${params.OLD_DC_NODES}\\""
                        def new_dc_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--new-dc-query \\"${params.NEW_DC_QUERY}\\"" : "--new-dc-nodes \\"${params.NEW_DC_NODES}\\""
                        sh """
                            chmod +x ./scripts/join-cassandra-dcs.sh
                            ./scripts/join-cassandra-dcs.sh \${old_dc_arg} \${new_dc_arg} --old-dc-name "\${params.OLD_DC_NAME}" --new-dc-name "\${params.NEW_DC_NAME}"
                        """
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Join', joinDcsJobScript)


// --- Split Datacenters Job ---
def splitDcsJobScript = """
pipeline {
    agent any
    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'DC1_QUERY', defaultValue: '-r role_cassandra_pfpt -d us-east-1', description: 'QV Query for first datacenter nodes.')
        string(name: 'DC2_QUERY', defaultValue: '-r role_cassandra_pfpt -d eu-west-1', description: 'QV Query for second datacenter nodes.')
        string(name: 'DC1_NODES', defaultValue: '', description: 'Node list for first datacenter.')
        string(name: 'DC2_NODES', defaultValue: '', description: 'Node list for second datacenter.')
        string(name: 'DC1_NAME', defaultValue: 'us-east-1', description: 'Cassandra name of the first datacenter.')
        string(name: 'DC2_NAME', defaultValue: 'eu-west-1', description: 'Cassandra name of the second datacenter.')
    }
    stages {
        stage('Execute Split Operation') {
            steps {
                dir('${projectPath}') {
                    script {
                        def dc1_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--dc1-query \\"${params.DC1_QUERY}\\"" : "--dc1-nodes \\"${params.DC1_NODES}\\""
                        def dc2_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--dc2-query \\"${params.DC2_QUERY}\\"" : "--dc2-nodes \\"${params.DC2_NODES}\\""
                        sh """
                            chmod +x ./scripts/split-cassandra-dcs.sh
                            ./scripts/split-cassandra-dcs.sh \${dc1_arg} \${dc2_arg} --dc1-name "\${params.DC1_NAME}" --dc2-name "\${params.DC2_NAME}"
                        """
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Split', splitDcsJobScript)


// --- Rename Cluster Job ---
def renameClusterJobScript = """
pipeline {
    agent any
    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'QV_QUERY', defaultValue: '-r role_cassandra_pfpt -d us-east-1', description: 'QV Query for ALL nodes in the cluster.')
        string(name: 'NODE_LIST', defaultValue: '', description: 'Node list for ALL nodes in the cluster.')
        string(name: 'OLD_NAME', defaultValue: 'MyProductionCluster', description: 'The CURRENT cluster name.')
        string(name: 'NEW_NAME', defaultValue: 'MyPrimaryCluster', description: 'The NEW desired cluster name.')
    }
    stages {
        stage('Execute Rename Operation') {
            steps {
                dir('${projectPath}') {
                    script {
                        def node_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--qv-query \\"${params.QV_QUERY}\\"" : "--nodes \\"${params.NODE_LIST}\\""
                        sh """
                            chmod +x ./scripts/rename-cassandra-cluster.sh
                            ./scripts/rename-cassandra-cluster.sh \${node_arg} --old-name "\${params.OLD_NAME}" --new-name "\${params.NEW_NAME}"
                        """
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Rename', renameClusterJobScript)


// --- Compaction Job ---
def compactionJobScript = """
pipeline {
    agent any
    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'QV_QUERY', defaultValue: '-r role_cassandra_pfpt', description: 'The QV query to select nodes.')
        string(name: 'NODE_LIST', defaultValue: '', description: 'A comma-separated list of nodes.')
        string(name: 'KEYSPACE', defaultValue: '', description: '(Optional) The keyspace to compact. Leave blank for all.')
        string(name: 'TABLES', defaultValue: '', description: '(Optional) Space-separated list of tables. Requires KEYSPACE.')
        string(name: 'NODETOOL_OPTIONS', defaultValue: '', description: '(Optional) Extra options to pass to nodetool compact (e.g., "--split-output").')
        booleanParam(name: 'PARALLEL', defaultValue: false, description: 'Run compaction in parallel on all nodes?')
    }
    stages {
        stage('Execute Compaction') {
            steps {
                dir('${projectPath}') {
                    script {
                        def node_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--qv-query \\"${params.QV_QUERY}\\"" : "--nodes \\"${params.NODE_LIST}\\""
                        def parallel_flag = params.PARALLEL ? '--parallel' : ''
                        
                        def cass_ops_command = "sudo /usr/local/bin/cass-ops compact"
                        if (params.KEYSPACE) {
                            cass_ops_command += " -k \${params.KEYSPACE}"
                        }
                        if (params.TABLES) {
                            params.TABLES.split(' ').each { table ->
                                cass_ops_command += " -t \${table}"
                            }
                        }
                        if (params.NODETOOL_OPTIONS) {
                            cass_ops_command += " --nodetool-options \\"\${params.NODETOOL_OPTIONS}\\""
                        }

                        sh "chmod +x ./scripts/cassy.sh && ./scripts/cassy.sh \${node_arg} -c \\"\${cass_ops_command}\\" \${parallel_flag}"
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Compaction', compactionJobScript)

// --- Garbage Collect Job ---
def garbageCollectJobScript = """
pipeline {
    agent any
    parameters {
        choice(name: 'NODE_SELECTION_METHOD', choices: ['QV_QUERY', 'NODE_LIST'], description: 'Method for selecting target nodes.')
        string(name: 'QV_QUERY', defaultValue: '-r role_cassandra_pfpt', description: 'The QV query to select nodes.')
        string(name: 'NODE_LIST', defaultValue: '', description: 'A comma-separated list of nodes.')
        string(name: 'KEYSPACE', defaultValue: '', description: '(Optional) The keyspace to run GC on. Leave blank for all.')
        string(name: 'TABLES', defaultValue: '', description: '(Optional) Space-separated list of tables. Requires KEYSPACE.')
        string(name: 'NODETOOL_OPTIONS', defaultValue: '', description: '(Optional) Extra options for nodetool garbagecollect (e.g., "-g CELL").')
        booleanParam(name: 'PARALLEL', defaultValue: false, description: 'Run garbage collection in parallel on all nodes?')
    }
    stages {
        stage('Execute Garbage Collect') {
            steps {
                dir('${projectPath}') {
                    script {
                        def node_arg = (params.NODE_SELECTION_METHOD == 'QV_QUERY') ? "--qv-query \\"${params.QV_QUERY}\\"" : "--nodes \\"${params.NODE_LIST}\\""
                        def parallel_flag = params.PARALLEL ? '--parallel' : ''

                        def cass_ops_command = "sudo /usr/local/bin/cass-ops garbage-collect"
                        if (params.KEYSPACE) {
                            cass_ops_command += " -k \${params.KEYSPACE}"
                        }
                        if (params.TABLES) {
                            params.TABLES.split(' ').each { table ->
                                cass_ops_command += " -t \${table}"
                            }
                        }
                        if (params.NODETOOL_OPTIONS) {
                            cass_ops_command += " --nodetool-options \\"\${params.NODETOOL_OPTIONS}\\""
                        }

                        sh "chmod +x ./scripts/cassy.sh && ./scripts/cassy.sh \${node_arg} -c \\"\${cass_ops_command}\\" \${parallel_flag}"
                    }
                }
            }
        }
    }
}
"""
createPipelineJob('Cassandra - Garbage-Collect', garbageCollectJobScript)
